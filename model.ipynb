{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model operationalization via s2i\n",
    "\n",
    "It's possible to use [s2i](https://github.com/openshift/source-to-image) to operationalize models that have been trained in a notebook.  The relevant builder image is [here](https://github.com/willb/simple-model-s2i); it works best if you follow some basic conventions.  The rest of this notebook will demonstrate these conventions with a simple example.\n",
    "\n",
    "## requirements\n",
    "\n",
    "The first convention to follow is declaring your model's requirements as a list of lists in a variable called `requirements`.  The s2i builder will use these to generate a `requirements.txt` file, which it will install while building an image.  This step is optional, but it is necessary if your model will depend on any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [[\"numpy\", \"1.15\"], [\"scikit-learn\", \"0.19.2\"], [\"scipy\", \"1.0.1\"], \n",
    "                [\"boto3\",\"1.9.112\"],[\"pandas\", \"0.19.2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model code\n",
    "\n",
    "Your model training code can just appear in this notebook as it would in any other.  Note that the s2i build process will execute every cell in the notebook in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = 2\n",
    "randos = np.random.random((40000,DIMENSIONS))\n",
    "labels = list(np.zeros(20000)) + list(np.ones(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = \"MICHAEL-DATA-ODSC\"\n",
    "conn = boto3.client(service_name='s3',\n",
    "        aws_access_key_id= os.environ.get(\"AWS_ACCESS_KEY_ID\")  ,\n",
    "        aws_secret_access_key= os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        endpoint_url= os.environ.get(\"S3_ENDPOINT_URL\"))\n",
    "\n",
    "obj = conn.get_object(Bucket=my_bucket, Key='demo-data/creditcard.csv')\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forrest Classifier\n",
      "test_acc = 0.9993960843796522\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, train_size=0.75)\n",
    "print(\"Random Forrest Classifier\")\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=10)\n",
    "model.fit(df_train.drop(['Time', 'Class'], axis=1),df_train['Class'])\n",
    "test_pred = model.predict(df_test.drop(['Time', 'Class'] ,axis=1))\n",
    "test_label = df_test['Class']\n",
    "test_acc = np.sum(test_pred==test_label) / len(test_pred)\n",
    "print(f'test_acc = {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = df_train.drop(['Time', 'Class'], axis=1).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmodel = KMeans(n_clusters=2).fit(randos)\n",
    "#kmodel = RandomForestClassifier().fit(randos,labels)\n",
    "kmodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate and predict\n",
    "\n",
    "Given a trained model, you simply need to provide two functions:\n",
    "\n",
    "* `predictor`, which will make a single prediction from a single sample, and\n",
    "* `validator`, which will return `True` if a single sample is of the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(x):\n",
    "    return kmodel.predict([x]).tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`validator` is optional, but it will make your model service easier to use.  If you don't provide one, your model service will accept any input, which will likely lead to confusing error messages (i.e., crashes somewhere in the `predictor`) if your model service is called with bogus input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validator(x):\n",
    "    return len(x) == DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
